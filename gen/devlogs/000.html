<!doctype html>
<html>
<head>
<title>The First Devlog</title>
<link rel="stylesheet" href="../main.css">
</head>
<body>
<div id="content">
<div id="navbar">
<a id="logo" href="../index.html">
<svg width="9.2622mm" height="9.2616mm" version="1.1" viewBox="0 0 9.2622 9.2616" xmlns="http://www.w3.org/2000/svg"> <g transform="translate(58.868 -264.01)">  <path transform="scale(.26458)" d="m-221.53 998.06c-0.4155 0-0.75 0.33452-0.75 0.75v33.07c0 0.4155 0.33451 0.75 0.75 0.75h33.072c0.4155 0 0.75-0.3345 0.75-0.75v-33.07c0-0.41552-0.33451-0.75-0.75-0.75zm0.17968 3.7402h13.076v0.7227c-1.224 0.1171-2.0182 0.3253-2.3828 0.625-0.36458 0.2994-0.54687 0.6836-0.54687 1.1523 0 0.6511 0.29948 1.6667 0.89844 3.0469l6.7383 15.527 6.25-15.332c0.61197-1.5104 0.91797-2.5586 0.91797-3.1445 0-0.3776-0.1888-0.7356-0.56641-1.0742-0.3776-0.3516-1.0156-0.599-1.9141-0.7422-0.0651-0.013-0.17579-0.032-0.33203-0.059h-0.77149v-0.7227h11.346v0.7227h-0.95703c-1.0807 0-1.8685 0.319-2.3633 0.957-0.32551 0.4166-0.48633 1.4193-0.48633 3.0078v17.109c0 1.3411 0.0846 2.2266 0.25391 2.6563 0.13022 0.3254 0.4017 0.6055 0.81836 0.8398 0.55989 0.3126 1.1524 0.4687 1.7773 0.4687h0.95703v0.7227h-11.346v-0.7227h0.9375c1.0938 0 1.888-0.319 2.3828-0.957 0.31251-0.4166 0.46875-1.4193 0.46875-3.0078v-14.363l-8.0566 19.656h-0.72265l-8.8379-20.338v15.045c0 1.3411 0.0846 2.2266 0.2539 2.6563 0.13022 0.3254 0.40367 0.6055 0.82031 0.8398 0.55991 0.3126 1.1524 0.4687 1.7774 0.4687h0.95703v0.7227h-11.348v-0.7227h0.9375c1.0938 0 1.888-0.319 2.3828-0.957 0.3125-0.4166 0.46875-1.4193 0.46875-3.0078v-17.109c0-1.3411-0.0846-2.2265-0.25391-2.6562-0.13022-0.3255-0.39715-0.6055-0.80078-0.8399-0.57292-0.3126-1.1719-0.4687-1.7969-0.4687h-0.9375z" stroke-width=".43432"/> </g></svg></a>
<ul>
<li><a href="../log_index.html">devlogs</a></li>
<li class="navbar_li_divider">|</li>
<li><a href="../doc_index.html">docs</a></li>
<li class="navbar_li_divider">|</li>
<li><a href="https://github.com/Soimn/mm">GitHub</a></li><li class="navbar_li_divider">|</li>
<li><a href="../about.html">About</a></li></ul>
</div>
<h1 id="title">The First Devlog</h1>
<p id="date">2022.03.20</p>
<span class="b_text">IMPORTANT NOTE:</span> This 'devlog' is very opinionated and not very scientific (or even logically valid and quite stupid at some points), as the arguments made are based mostly on 'common sense'. I wrote this mostly for my own sake, as to make it clear what I really want from the language and how it should work. Something that should also be noted is that I grew very tired of writing a devlog instead of writing code, which can be seen in the deterioration of my writing and arguments from the Progamming and Coding section and onward.

<br>
<br>The title of this devlog may be a bit deceiving, as this log was not written before starting, or anywhere near the start of, the project. This iteration of the project began in November 2021, and the compiler for the language has been completed up to, but not including, iterative type checking. The reason I say 'this' iteration, is that the project has been rebooted countless times since the first iteration in 2019, and is going to undergo another reboot during this devlog. So, in a sense, this is the last devlog for the previous iteration, as well as the first for the new iteration.

<br>Anyway, the motivation for this project is largely to take a step away from the rat's nest of complexity that modern software has become, mainly by making a language with accompanying tools that try to alleviate these problems.

<br>
<br><h3>Project history and Approach
</h3>The set of goals has varied for each iteration, some more than others. At first, the main goal was to create a C compiler with metaprogramming capabilities similar to Jai. This proved out to be difficult, as parsing C is a horrible experience, and modifying its grammar to support multiple return values, type inference and other shenanigans made it way worse. Some later iteration scrapped the C syntax, in favor of a syntax similar to Odin/Jai, and then the main goal was to make metaprogramming maximally powerful. This, in my mind, implied that the language should be made as simple as possible, such that metaprogramming only had to deal with its own complexity, and not some arbitrary cruft in the language design. However, this ended up getting out of hand, making the language concepts deviate too much from what actually runs on hardware. The next iteration then focused on making every decision grounded in reality, by examining a selection of target systems, and trying to construct a common model. From the selection of hardware I made, this became a rat's nest of complexity. Trying to learn from my mistakes, the last iteration made focused on making a simple bare-bones prototype of something I thought would work, and then evaluating and iterating upon that prototype. This is kind of the most obvious tactic to solving a problem, since you never truly know how it should be solved before actually solving it. I always knew this, but I kept thinking I wanted to avoid the common mistake in software of making something that seems to work, and then bunching on a ton of complexity to handle new features and fix bugs. The way I thought was the correct way of avoiding this was to thoroughly examine the problem and design everything neatly to solve that specific problem. However, as previously stated, you never truly know how a problem should be solved before actually solving it, which ended biting me hard. I kept on wasting way too much time on planning, afraid of writing something ad hoc that works, since it might not be the optimal solution I was looking for. Which is kind of weird, since I am not afraid of rewriting everything, as I have countless times (in total I have pushed 89 thousand lines of code to github, and removed 76 thousand, over the course of three iterations). What I should have learned is that the problem of tacking on complexity and making a mess, doesn't come from not planning enough, it comes from not discarding prototypes, and building upon them instead. Since I am clearly not afraid of scrapping everything, as opposed to some project manager in a company, I should leverage this, and stop "overplanning". The next iteration of the project will therefore prioritize making quick prototypes, instead of simulating a design committee with choice paralysis.

<br><h3>Foundation
</h3>Now that the general development approach has been decided, what is it that I will be making? This seems like a weird question, since this is clearly a project devoted to making a programming language and compiler. However, making a programming language is not a very specific goal, as it leaves a bunch of questions like is it compiled or interpreted, is it object oriented or maybe functional, and generally what is actually accomplished by making yet another random programming language? To answer some of these questions, I will be taking a step back and evaluate what a programming language should be, what I want to accomplish by making one and what a programming language even is to begin with.

<br><h4>What is a programming language?
</h4>According to the Wikipedia page on the <a href="https://en.wikipedia.org/wiki/History_of_programming_languages">history of programming languages</a>
<div class="code">"It was eventually realized that programming in assembly language required a great deal of intellectual effort"
</div>
Which seems to imply that the reason programming languages came about was that programming in assembly was too hard, and that an easier way of programming computers would come in the form of programming languages. Viewed in a positive light, this statement seems like it makes sense, and would sort of imply that programming languages are a superior lossless representation of machine code that is easier understood by humans. However, when you think about how quality software is made today, a lot of it comes down to having fine grain control of what the computer does. Which, in my mind, is not very far away from programming in assembly. So in short, making software using programming languages is superior to assembly, except for when you want to make software of high quality. Then why was programming in assembly viewed as a problem, when it seems like assembly clearly is useful for some parts of programming?

<br><h5>Control
</h5>In my opinion, it all comes down to control. Assembly tends to offer the programmer more control than other programming languages. Control over where values live and which operations are used (to an extent, since machine code has become more of a suggestion to modern processors, as opposed to a list of strict instructions). This control gives a programmer the power needed to optimize routines, and crank out every single drop of that sweet sweet performance juice (one could argue this is also possible with intrinsics, however they are even more of a suggestion than assembly). However, this control also comes with a responsibility, a responsibility to utilize the computing resources effectively. With a higher level programming language, this responsibility is often shifted more towards the language, which makes easier to write non optimal programs that still run relatively efficiently. This is essentially the advantage of programming languages, they allow you to gain ease of use in exchange for control. 

<br>
<br><h5>"Zero Cost Abstractions"
</h5>When people talk about so-called "zero cost abstractions", they often try to explain why a certain language construct abstracts away a bunch of complexity, and allows a programmer to express logic in simpler terms, while still keeping the "performance" of writing it all out in assembly. This is basically never true, "zero cost abstractions" always lose something, since that is what it means to abstract <span class="i_text">away</span> something. Abstraction will always reduce control. Even "trivial abstractions" such as replacing opcodes with mnemonics reduces control. Now the programmer cannot reason about the relative order of instructions given by their opcode's value without looking up the value for each mnemonic and breaking the abstraction. This might seem like a silly example, as knowing the relative order of instructions by their opcode seems useless, and mnemonics are surely easier to remember than a string of digits. However, knowing the opcode for each instruction is incredibly useful when writing self modifying code. This nicely illustrates the point that abstraction is always at the loss of something, and also leads up to the next point.

<br>
<br><h5>Breaking Abstractions
</h5>So far it would seem like every programmer is stuck with a choice between not having enough control to write optimal software, and too much control with not enough skill, or experience, to properly utilize it. However, this is not the only option. The previous paragraph may have made breaking the "instruction mnemonic" abstraction seem like something bad, but this might actually be the best solution we have. Surely writing software with maximal control would be the optimal solution for an all-knowing intelligence. However, as humans are famously bad at always making optimal decisions, programming with maximal control may not be the best solution. The obvious answer to this would be mixing levels of abstractions when it is deemed necessary. This would allow programming at the "lowest level" of abstraction the programmer is confident in, while still keeping the option for maximal control when deemed necessary. While sounding wonderful, this introduces a whole new class of problems relating to cooperation between layers of abstraction, as an assembly routine is useless if it cannot read the value of its parameters, due to them "being stuck at a higher level of abstraction".

<br><h5>Programming languages
</h5>At this point, a somewhat useful definition of a programming language can be formed.
<div class="code">A programming language is an, often multi-layered, abstraction of machine code
</div>
Which is obvious, but what is useful about this definition is that is illustrates a problem with most languages, namely that it is an <span class="b_text">abstraction</span>. By being an abstraction, every language will therefore inherit the problem of managing control. Despite this being a glaring problem, when the goal is to write optimal software, it would seem like languages these days are praised for their abstractions. From the object-oriented languages such as Java, to scripting languages like Python, they are all praised for their "ease of use", and accompanying implied ease of writing robust and secure software. This is just absurd, as what people are basically saying is that these languages make software easier to write, more robust and secure, by allowing the programmer not to care how the program works. Which is kind of bonkers, because how would we know that the program is actually more robust and secure if nobody knows how it actually works. And if someone does know how it works, and can vouch for these claims, why couldn't they write it instead, as they clearly know more about the program than the actual programmers who wrote it. What these languages offer is a (mostly) unbreakable high level abstraction of machine code, and if you agree with the previous paragraph, this is obviously a stupid design if writing optimal software is the goal. However, you still have to remember how we got here, which was initially by quoting Wikipedia stating that assembly is often too hard to work with.

<br>
<br>Then for a programming language to be a useful concept, I would propose the following: 
<div class="code">A programming language should be an arbitrarily high stack of abstractions over machine code that the programmer is in full control over, and allows them to seamlessly transition from one layer to another
</div>
Programming languages should therefore not restrict the programmer to an arbitrary level of abstraction, but rather provide the tools to build abstractions over the lowest possible level, and make transitioning between layers seamless.

<br><h4>Aspects of programming
</h4><h5>Programming And Coding
</h5>The words 'programming' and 'coding' are often used interchangeably when referring to the work of a software engineer. This is not how I personally use these words, as I view these as labels for two distinct parts of a software engineer's job. I view programming as the act of solving problems and phrasing the solutions in ways a computer could run. Coding is to me the act of encoding that solution, which is basically the same as translating decimal to binary, just with different rules. Now, what does this have to do with the previous topic of programming language design and abstractions? The gist of it is that when programming, the 'programmer', has a mental model of how the program should work, and how it solves the problem at hand. This model is then encoded into, most often, text, and can be decoded again at a later time. There are now two different representations of the model for how the program should work, and this causes some problems.

<br>
<br>Firstly, lossy, or faulty, encoding or decoding results in a disconnect between the models. The most obvious example of this is the programmer making a mistake when encoding the solution, and either causing a syntax error, something more grave. Moreover, since humans are flawed, the process of encoding and decoding mental models is both lossy and faulty, making collaborating with several people, and even oneself, hard. This is because the mental model of each person may differ greatly from both every other person's model and the encoded model.

<br>
<br>Secondly, the coding format may indirectly affect the 'solution space' explored by the programmer. Ideas that are harder to encode will naturally be avoided when the solver of the problem at hand, and the encoder of the mental model, are the same person. This does not only cause problems for lone programmers, but will also ultimately affect the society, making some solutions unfavorable simply because they are harder to encode.

<br>
<br>Finally, the efficiency of the encoding and decoding process is largely dictated by how much the code format differs from the mental model. Which is only natural, as a coding format that is an exact copy of the mental model's format does not require any work to encode and decode.

<br>
<br>To alleviate these problems, the coding format should be chosen carefully to match 'the mental model' as closely as possible. However, for a language to be useful to a group of people, the coding format must be chosen to match their mental models, which becomes increasingly more impossible as the group size expands. Furthermore, the coding format must change over time, as people tend to learn and restructure how they think. This approach also has the problem of penalizing experienced problem solvers, in favor of matching more closely to the mental model of beginners, given they are the increasing majority.

<br>
<br>The problem is most often 'solved' by a committee designing a format and dictating that everyone should adjust their mental model to match. This is often a terrible approach, as it forces people to think in ways that are often inefficient and 'creatively limiting'. Furthermore, the dictated mental model often tend to favor matching beginners, rather than more experienced developers. Which is all well when it can be used as a steppingstone, but is vastly unfavorable for the advancement of technology. However, the alternative of adapting the coding format to every single person is even worse off. This approach makes it almost impossible for any two people to work together, which is frankly a lot worse than lowering the efficiency, or limiting creativity. The compromise I would propose is to base the coding format and intended mental model on the basics of how a computer works, along with well established constructs in programming languages. This is not the most optimal solution, by far, but it would be familiar to experienced programmers, as well as providing beginners with an easy introduction by learning the basics of computers.

<br>
<br>Now, proposing that something should follow 'established principles and basic theory of a computer', without elaborating on exactly what that means, is almost like saying 'yes, I too like good compromises', while thinking to yourself that you are much smarter, and experienced, than everyone around you. Therefore, I will at least make an attempt at defining things. Also as a preface, I must make it clear that my own experience consists mostly of self-taught game programming in C#, C++ and C with nothing even resembling a half-finished product to speak of.

<br><h6>The Basics of a Computer
</h6>As previously stated, programming can be thought of as solving problems and phrasing the solution in a way that is executable by a computer. This implies that the abstract properties of a computer are fundamental to the act of programming. Furthermore, as real life metrics, like time used and power consumed, are of interest, not only are the abstract properties important, but also the actual inner workings, as they dictate these metrics. 

<br>
<br>However, once again, we encounter the tradeoff of control vs complexity. To optimally utilize the computer requires a lot of control, which requires the programmer knowing every tiny little detail about the computer. Furthermore, the programmer needs to know every tiny little detail about every single computer the program is to be run on. Which in turn consists of countless combinations of components, which are, most often, of high complexity themselves. The tactic to 'solve' this problem is once again to recognize some commonalities between computers and make an abstraction that is as close to this, while still being general enough to hold true for every system. 

<br>
<br>However, on modern systems, this is much harder than one would think, as even simple things like arithmetic behaves vastly different on x86 and ARM (mostly related to exceptions and bitwise shifts), making constructing a layer of abstraction with minimal loss of detail very hard. On top of that, I, the author, have no experience, whatsoever, with the internals of a modern processor (past the 'basics'), and that's not even talking about the monstrosity that is a GPU. With the problem being a tough tradeoff, and me lacking experience, the solution is in a way doomed to be awful. Moreover, for the solution to be any good, a beginner has to be able to learn it easily without having to study computers for a decade. 

<br>
<br>Before trying to make an abstract machine that complies with these demands, it would be wise to pick the target hardware the language should support. Since the language is mainly concerned with systems level game programming, hardware on all major consoles and common pc builds must be supported. x64 dominates this market, with some systems running x86 or ARMv8-A. For GPUs the situation is a bit more complicated. As far as I know, there isn't a convenient way of talking to the GPU without going through a vendor specific proprietary driver. This, along with other functionality locked by the OS, ends up restricting the model to essentially a CPU running an OS with drivers. This isn't the optimal, super simple, all encompassing model that would be very nice to have, but is probably the only reasonable model given the restrictions.

<br>
<br>The task is then to create a model for x86, x64 and ARMv8-A based systems paired with the common operating systems, including console operating systems. From what I have gathered by glossing over the wikichip pages for <a href="https://en.wikichip.org/wiki/amd/microarchitectures/zen">zen</a> and <a href="https://en.wikichip.org/wiki/intel/microarchitectures/kaby_lake">kaby lake</a>, as well as the arm developer <a href="https://developer.arm.com/documentation/ddi0488/h/introduction/about-the-cortex-a57-processor">block diagram</a> of a cortex-a57, modern processors tend to look like this:

<br><figure style="margin: 0; padding: 0;">
	<img src="abstract_cpu.svg" style="maring: 0; padding: 0; width: 100%;">
	<figcaption style="margin: 0; padding: 0; text-align: right; font-size: 0.8em;">Figure 1: A crude abstract model of a quad-core cpu</figcaption>
</figure>

<br>Where essentially the CPU consists of several cores wired together, and connected to a shared memory controller, io and cache. The individual cores can be split in three different components: the front end, the execution engine and the memory subsystem. The front end takes care of fetching and decoding instructions to feed the execution engine. The branch predictor takes care of predicting which instructions should be prioritized, the instruction cache serves as the primary cache for instruction fetching, and the instruction translation lookaside buffer caches TLB lookups done for instructions. The execution engine consists mostly of a scheduling unit that feed the instructions from the front end into several execution units, that do the work specified by the instruction. Registers needed by these instructions are fetched from the register file, which is essentially an array of register sized values with the mapping from register name to array index stored in the register allocation table. The scheduler feeds the instructions (usually micro ops at this stage) into the execution units out of order, and the results are then put back in order in the retirement buffer.

<br>The main differences between the architectures, observable by the programmer, are the specifics of how the execution units work. There are differences in how many units there are and which operations the units support, which are mostly of concern when dealing with optimization, which requires programming with the target system in mind, which would then not concern the abstract model. What does concern the model is how the operations work and what side effects they produce. All architectures in question support 2's complement integer and IEEE 754 floating point arithmetic, however they differ in how they deal with edge cases. Most of the edge cases I have found are illegal operations, however there are some that seem like they should be legal. One of these is a shift instruction with a shift amount greater than the width of the shift operand. Intuitively, this would just shift "more", following the pattern of shifting by width - 1 times until the shift amount is less than width, and then shifting by the remainder. This is however not what happens, as the shift amount is masked before the shift with 0x1F (bottom five bits little endian), which implies any shift amount where the remainder of a division by 32 is 0 will result in no shift, and a shift amount of 33 will shift one place instead. Another weird edge case is integer division by 0. On x86 and x64 a divide error is issued when dividing by zero with no result (that I could find at least), but on ARMv8-A the result is 0, with no trap issued. This is kind of hard to model around, as there is no common result of the operation. The laziness of C comes in handy in these situations, where we could just flag these edge cases as architecture dependent behavior and call it done.

<br><h6>Established Language Constructs
</h6>Although there have been made countless programming languages, most languages today fall into three categories: languages that build upon C, languages that build upon Lisp or any other non systems programming language, and esoteric languages. This is a crude simplification, but, truth is, most modern (I hate that word) systems programming languages are inspired by C. Of course there are languages that inspired C (B is one of them if I remember correctly), but the point is that most systems and application programming languages today, like C++, Java, C#, Rust, Odin, Pyhton and even Javascript (yuck), all share a large resemblance with C. C has therefore been huge factor in shaping how we think about, teach and view programming. Some of the major ideas that have survived are: structured programming, basic operators, procedures, types, variables and pointers. The only additions (I can think of) that C makes upon this is text processing with the preprocessor, goto, switch and inline assembly. Starting of the language will be based on the major ideas. 

<br><h5>Raising Walls
</h5>"Raising walls" is what I call segmenting code. This could be separating code into several scopes, functions, files or even libraries. The point is that you are making a mental distinction between two different pieces of encoded logic and enforcing some kind of encapsulation-esque ruleset on them. "Raising walls" is a tool,  with its own set of tradeoffs that depend on the specific circumstances. And unlike OOP, I would not advocate encapsulating code willy-nilly. This is because encapsulating code, or "raising walls" around the code, affects more than just "code quality", it affects the solution space. This concept is explained well in Casey Muratori's <a href="https://youtu.be/5IUj1EZwpJY">The Only Unbrekable Law</a>. When designing the import system for one of the last iterations of the language, this was something I became aware of. I was basing the import system in how it works in Odin, which requires that each file is tagged with a package they belong to. This ensures nice properties like stable linking names that don't collide, but it also has the unfortunate property that it encourages segmenting a program into several packages, especially since the main package cannot contain more than one file (since there are no C like include directives). This is of course not a problem if you don't think of spreading your code over several packages as making hard boundaries between them, but it does still encourage thinking in this way, and since there is no "softer" version of this, like including files, you are forced early on in a project to take a stance about how the project should be organized, which is the worst point to enforce rules, since you know the least about the project.

<br><h5>User Interfaces
</h5>Although I haven't improved much in terms of software architecture, one aspect of programming that I notice an improvement in is making code readable. This is not by following any of those stupid conventions on formatting, but by structuring code in a way that is most helpful when reading. This includes simple things like spacing things that don't relate to each other further away, gathering things that do relate, and using consistent names that carry information on type and usage. But also gathering information that is relevant for a block of code as close to the start of the block as possible, and choosing the shortest or most relevant block of an if-else chain and placing it closest to the condition. Furthermore, I use three different styles of commenting, the regular //, a /// and a ////. This is used to affect the syntax highlighting of the comment, which I then use to markup code. // is used for normal comments, /// is used for headings and has a lighter and bolder color, while //// is used to mark errors and is a very prominent orange color. This allows me to quickly identify blocks of code and find error out conditions to orient myself when reading.

<br>Thinking about this, I realized that everything I do follows the Gestalt principles of design. I gather things closer together to take advantage of the principle of proximity to indicate relation, and vice versa with spreading code apart. The same holds for structuring blocks of code, with the most "block relevant" parts first. Furthermore, the variable naming and comment coloring utilizes the principle of similarity. Although this is not a perfect relationship, I have realized that computer science should really take a look around at what other fields are doing and learning from them, because there is soooo much more to making code readable than some dumb lint script. Everything is about encoding intent and guiding the reader.

<br><h5>Concept Art
</h5>Another aspect of programming that should be revised in light of how other fields deal with the problem is workflow and planning. To me, architecting a program is very similar to concept art, or character design. Concept art is made for the purpose of guiding design, by realizing different design options such that they can be compared. The workflow is very centered around iteration, where several variations of an initial design description are made, then a couple of them are selected as candidates and used to seed new variations until the final design is decided. This has the very real advantage of making the effect of each decision measurable, which in turn makes deciding where to go next easier. This is typically not how software is designed. A typical software workflow starts with defining and breaking up the task, then completing each fragment and stitching each piece together. This seems like an effective approach until you start to consider the implications of defining and breaking up the problem at the very start. You never truly know what the best solution to a problem is without having tried it out. This means you never truly know if your choices are optimal until you have made them, measured their effects, and compared them against every other possible choice. To assume someone, who hasn't solved a problem is able to optimally define and break up the problem into tasks, is just bonkers. This is essentially the same as making decisions about optimization purely based on guesses about their effects, which the industry has coined premature optimization, and is dead set on hating on it every chance it gets. Now of course more iterations costs more time and resources, but if the goal is to create the best product possible with a set amount of resources, I would place way more importance on experimenting and iterating on different parts of the design in order to gather the necessary information to make good decisions about it.

<br><h5>Daredevil Programming
</h5>The last aspect of programming I want to bring up is what I, as of writing this sentence, call "Daredevil programming". This is a term that describes the current lack of programmer "visibility" in a lot of tools and languages. An example of this is compiler magic and undefined behavior, where the compiler can do whatever it wants to, without signaling the programmer. Another example is the lack of good visualization tools in debuggers, making the programmer either resolve to using a third party tool and painstakingly manage the transferring the state and building models, or construct this model in their heads with the raw data readout from the debugger. Each of these examples show a lack of direct communication with the programmer, making them resort to indirect observations, and since the Daredevil comic book character sees only indirectly via sound and smell, I thought the name was fitting. As I see it, the only real solution to these problems is making compilers simpler and more transparent about how they work and which decisions they have made, as well as beefing up debuggers, since the state most debuggers are in today is quite sad (look at WhiteBox for an example of what a more beefy debugger could do).

<br><h3>Designing the Language
</h3>TODO: spec and reasoning

<br>
<br>Tools:
terminal, text editor, version control system, visual debugger with graphics debugging, packet sniffing and other shit</div>
<div id="footer"></div>
</body>
</html>