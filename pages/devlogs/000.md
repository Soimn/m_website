$
title: The First Devlog
date: 2022.03.20
$
**NOTE:** This 'devlog' very opinionated and not very scientific, as the arguments made are based mostly on 'common sense'. I wrote this mostly for my own sake, as to make it clear what I really want from the language and how it should work.


The title of this devlog may be a bit deceiving, as this log was not written before starting, or anywhere near the start of, the project. This iteration of the project began in November 2021, and the compiler for the language has been completed up to, but not including, iterative type checking. The reason I say 'this' iteration, is that the project has been rebooted countless times since the first iteration in 2019, and is going to undergo another reboot during this devlog. So, in a sense, this is the last devlog for the previous iteration, as well as the first for the new iteration.

Anyway, the motivation for this project is largely to take a step away from the rat's nest of complexity that modern software has become, mainly by making a language with accompanying tools that try to alleviate these problems.


## Project history and Approach
The set of goals has varied for each iteration, some more than others. At first, the main goal was to create a C compiler with metaprogramming capabilities similar to Jai. This proved out to be difficult, as parsing C is a horrible experience, and modifying its grammar to support multiple return values, type inference and other shenanigans made it way worse. Some later iteration scrapped the C syntax, in favor of a syntax similar to Odin/Jai, and then the main goal was to make metaprogramming maximally powerful. This, in my mind, implied that the language should be made as simple as possible, such that metaprogramming only had to deal with its own complexity, and not some arbitrary cruft in the language design. However, this ended up getting out of hand, making the language concepts deviate too much from what actually runs on hardware. The next iteration then focused on making every decision grounded in reality, by examining a selection of target systems, and trying to construct a common model. From the selection of hardware I made, this became a rat's nest of complexity. Trying to learn from my mistakes, the last iteration made focused on making a simple bare-bones prototype of something I thought would work, and then evaluating and iterating upon that prototype. This is kind of the most obvious tactic to solving a problem, since you never truly know how it should be solved before actually solving it. I always knew this, but I kept thinking I wanted to avoid the common mistake in software of making something that seems to work, and then bunching on a ton of complexity to handle new features and fix bugs. The way I thought was the correct way of avoiding this was to thoroughly examine the problem and design everything neatly to solve that specific problem. However, as previously stated, you never truly know how a problem should be solved before actually solving it, which ended biting me hard. I kept on wasting way too much time on planning, afraid of writing something ad hoc that works, since it might not be the optimal solution I was looking for. Which is kind of weird, since I am not afraid of rewriting everything, as I have countless times (in total I have pushed 89 thousand lines of code to github, and removed 76 thousand, over the course of three iterations). What I should have learned is that the problem of tacking on complexity and making a mess, doesn't come from not planning enough, it comes from not discarding prototypes, and building upon them instead. Since I am clearly not afraid of scrapping everything, as opposed to some project manager in a company, I should leverage this, and stop "overplanning". The next iteration of the project will therefore prioritize making quick prototypes, instead of simulating a design committee with choice paralysis.

## Foundation
Now that the general development approach has been decided, what is it that I will be making? This seems like a weird question, since this is clearly a project devoted to making a programming language and compiler. However, making a programming language is not a very specific goal, as it leaves a bunch of questions like is it compiled or interpreted, is it object oriented or maybe functional, and generally what is actually accomplished by making yet another random programming language? To answer some of these questions, I will be taking a step back and evaluate what a programming language should be, what I want to accomplish by making one and what a programming language even is to begin with.

### What is a programming language?
According to the Wikipedia page on the [history of programming languages](https://en.wikipedia.org/wiki/History_of_programming_languages)
```
"It was eventually realized that programming in assembly language required a great deal of intellectual effort"
```
Which seems to imply that the reason programming languages came about was that programming in assembly was too hard, and that an easier way of programming computers would come in the form of programming languages. Viewed in a positive light, this statement seems like it makes sense, and would sort of imply that programming languages are a superior lossless representation of machine code that is easier understood by humans. However, when you think about how quality software is made today, a lot of it comes down to having fine grain control of what the computer does. Which, in my mind, is not very far away from programming in assembly. So in short, making software using programming languages is superior to assembly, except for when you want to make software of high quality. Then why was programming in assembly viewed as a problem, when it seems like assembly clearly is useful for some parts of programming?

#### Control
In my opinion, it all comes down to control. Assembly tends to offer the programmer more control than other programming languages. Control over where values live and which operations are used (to an extent, since machine code has become more of a suggestion to modern processors, as opposed to a list of strict instructions). This control gives a programmer the power needed to optimize routines, and crank out every single drop of that sweet sweet performance juice (one could argue this is also possible with intrinsics, however they are even more of a suggestion than assembly). However, this control also comes with a responsibility, a responsibility to utilize the computing resources effectively. With a higher level programming language, this responsibility is often shifted more towards the language, which makes easier to write non optimal programs that still run relatively efficiently. This is essentially the advantage of programming languages, they allow you to gain ease of use in exchange for control. 


#### "Zero Cost Abstractions"
When people talk about so-called "zero cost abstractions", they often try to explain why a certain language construct abstracts away a bunch of complexity, and allows a programmer to express logic in simpler terms, while still keeping the "performance" of writing it all out in assembly. This is basically never true, "zero cost abstractions" always lose something, since that is what it means to abstract *away* something. Abstraction will always reduce control. Even "trivial abstractions" such as replacing opcodes with mnemonics reduces control. Now the programmer cannot reason about the relative order of instructions given by their opcode's value without looking up the value for each mnemonic and breaking the abstraction. This might seem like a silly example, as knowing the relative order of instructions by their opcode seems useless, and mnemonics are surely easier to remember than a string of digits. However, knowing the opcode for each instruction is incredibly useful when writing self modifying code. This nicely illustrates the point that abstraction is always at the loss of something, and also leads up to the next point.


#### Breaking Abstractions
So far it would seem like every programmer is stuck with a choice between not having enough control to write optimal software, and too much control with not enough skill, or experience, to properly utilize it. However, this is not the only option. The previous paragraph may have made breaking the "instruction mnemonic" abstraction seem like something bad, but this might actually be the best solution we have. Surely writing software with maximal control would be the optimal solution for an all-knowing intelligence. However, as humans are famously bad at always making optimal decisions, programming with maximal control may not be the best solution. The obvious answer to this would be mixing levels of abstractions when it is deemed necessary. This would allow programming at the "lowest level" of abstraction the programmer is confident in, while still keeping the option for maximal control when deemed necessary. While sounding wonderful, this introduces a whole new class of problems relating to cooperation between layers of abstraction, as an assembly routine is useless if it cannot read the value of its parameters, due to them "being stuck at a higher level of abstraction".

#### Programming languages
At this point, a somewhat useful definition of a programming language can be formed.
```
A programming language is an, often multi-layered, abstraction of machine code
```
Which is obvious, but what is useful about this definition is that is illustrates a problem with most languages, namely that it is an **abstraction**. By being an abstraction, every language will therefore inherit the problem of managing control. Despite this being a glaring problem, when the goal is to write optimal software, it would seem like languages these days are praised for their abstractions. From the object-oriented languages such as Java, to scripting languages like Python, they are all praised for their "ease of use", and accompanying implied ease of writing robust and secure software. This is just absurd, as what people are basically saying is that these languages make software easier to write, more robust and secure, by allowing the programmer not to care how the program works. Which is kind of bonkers, because how would we know that the program is actually more robust and secure if nobody knows how it actually works. And if someone does know how it works, and can vouch for these claims, why couldn't they write it instead, as they clearly know more about the program than the actual programmers who wrote it. What these languages offer is a (mostly) unbreakable high level abstraction of machine code, and if you agree with the previous paragraph, this is obviously a stupid design if writing optimal software is the goal. However, you still have to remember how we got here, which was initially by quoting Wikipedia stating that assembly is often too hard to work with.

Then for a programming language to be a useful concept, I would propose the following: 
```
A programming language should be an arbitrarily high stack of abstractions over machine code that the programmer is in full control over, and allows them to seamlessly transition from one layer to another
```
Programming languages should therefore not restrict the programmer to an arbitrary level of abstraction, but rather provide the tools to build abstractions over the lowest possible level, and make transitioning between layers seamless.

### ----
#### Programming And Coding
The words 'programming' and 'coding' are often used interchangeably when referring to the work of a software engineer. This is not how I personally use these words, as I view these as labels for two distinct parts of a software engineer's job. I view programming as the act of solving problems and phrasing the solutions in ways a computer could run. Coding is to me the act of encoding that solution, which is basically the same as translating decimal to binary, just with different rules. Now, what does this have to do with the previous topic of programming language design and abstractions? The gist of it is that when programming, the 'programmer', has a mental model of how the program should work, and how it solves the problem at hand. This model is then encoded into, most often, text, and can be decoded again at a later time. There are now two different representations of the model for how the program should work, and this causes some problems.


Firstly, lossy, or faulty, encoding or decoding results in a disconnect between the models. The most obvious example of this is the programmer making a mistake when encoding the solution, and either causing a syntax error, something more grave. Moreover, since humans are flawed, the process of encoding and decoding mental models is both lossy and faulty, making collaborating with several people, and even oneself, hard. This is because the mental model of each person may differ greatly from both every other person's model and the encoded model.


Secondly, the coding format may indirectly affect the 'solution space' explored by the programmer. Ideas that are harder to encode will naturally be avoided when the solver of the problem at hand, and the encoder of the mental model, are the same person. This does not only cause problems for lone programmers, but will also ultimately affect the society, making some solutions unfavorable simply because they are harder to encode.


Finally, the efficiency of the encoding and decoding process is largely dictated by how much the code format differs from the mental model. Which is only natural, as a coding format that is an exact copy of the mental model's format does not require any work to encode and decode.


To alleviate these problems, the coding format should be chosen carefully to match 'the mental model' as closely as possible. However, for a language to be useful to a group of people, the coding format must be chosen to match their mental models, which becomes increasingly more impossible as the group size expands. Furthermore, the coding format must change over time, as people tend to learn and restructure how they think. This approach also has the problem of penalizing experienced problem solvers, in favor of matching more closely to the mental model of beginners, given they are the increasing majority.


The problem is most often 'solved' by a committee designing a format and dictating that everyone should adjust their mental model to match. This is often a terrible approach, as it forces people to think in ways that are often inefficient and 'creatively limiting'. Furthermore, the dictated mental model often tend to favor matching beginners, rather than more experienced developers. Which is all well when it can be used as a steppingstone, but is vastly unfavorable for the advancement of technology. However, the alternative of adapting the coding format to every single person is even worse off. This approach makes it almost impossible for any two people to work together, which is frankly a lot worse than lowering the efficiency, or limiting creativity. The compromise I would propose is to base the coding format and intended mental model on the basics of how a computer works, along with well established constructs in programming languages. This is not the most optimal solution, by far, but it would be familiar to experienced programmers, as well as providing beginners with an easy introduction by learning the basics of computers.


Now, proposing that something should follow 'established principles and basic theory of a computer', without elaborating on exactly what that means, is almost like saying 'yes, I too like good compromises', while thinking to yourself that you are much smarter, and experienced, than everyone around you. Also as a preface, I must make it clear that my own experience consists mostly of self-taught game programming in C\#, C++ and C with nothing even resembling a half-finished product to speak of.

##### The Basics of a Computer
As previously stated, programming can be thought of as solving problems and phrasing the solution in a way that is executable by a computer. This implies that the abstract properties of a computer are fundamental to the act of programming. Furthermore, as real life metrics, like time used and power consumed, are of interest, not only are the abstract properties important, but also the actual inner workings, as they dictate these metrics. 

However, once again, we encounter the tradeoff of control vs complexity. To optimally utilize the computer requires a lot of control, which requires the programmer knowing every tiny little detail about the computer. Furthermore, the programmer needs to know every tiny little detail about every single computer the program is to be run on. Which in turn consists of countless combinations of components, which are most often of high complexity themselves. The tactic to 'solve' this problem is once again to recognize some commonalities between computers and make an abstraction that is as close to this, while still being general enough to hold true for every system. 

However, on modern systems, this is much harder than one would think, as even simple things like arithmetic behaves vastly different on x86 and ARM (mostly related to exceptions and bitwise shifts), making constructing a layer of abstraction with minimal loss of detail very hard. On top of that, I, the author, have no experience, whatsoever, with the internals of a modern processor (past the 'basics'), and that's not even talking about the monstrosity that is a GPU. With the problem being a tough tradeoff, and me lacking experience, the solution is in a way doomed to be awful. Moreover, for the solution to be any good, a beginner has to be able to learn it easily without having to study computers for a decade. 

All this signals to me that the solution, which is an ugly tradeoff, is going back to 1975 and basing the model of a computer on systems like the NES. The model of a computer would then consist of a CPU 

##### Established Language Constructs

#### Raising Walls
TODO: Conway's Law

#### User Interfaces
TODO: Getsalt principles

#### Concept Art
TODO: Iteration and tweaking

#### Daredevil Programming
TODO: Visualization and Bret Victor, Undefined Behaviour and compiler magic

## Designing the Language
TODO: spec and reasoning